import json
import requests
import datetime
from openai import OpenAI

client = OpenAI()

# ------------------------------------------------------------------
# Remote data locations (same as before)
# ------------------------------------------------------------------
BASE_URL = "https://pos.kartingcentral.co.uk/home/download/pos2/pos2/popfinder/backend/data"

RULES_URL = f"{BASE_URL}/rules.txt"
NOTES_URL = f"{BASE_URL}/notes.txt"
PINS_URL  = f"{BASE_URL}/pins.json"
SEED_URL  = f"{BASE_URL}/seed_events.json"


# ------------------------------------------------------------------
# Helpers to read remote files
# ------------------------------------------------------------------
def load_remote(url: str) -> str:
    try:
        r = requests.get(url, timeout=8)
        r.raise_for_status()
        return r.text
    except Exception:
        return ""


def load_json_remote(url: str):
    try:
        r = requests.get(url, timeout=8)
        r.raise_for_status()
        return r.json()
    except Exception:
        return []


# ------------------------------------------------------------------
# Safety filters for dates / URLs
# ------------------------------------------------------------------
def url_seems_real(url: str) -> bool:
    """
    Very conservative URL sanity check.
    """
    if not isinstance(url, str) or len(url) < 10:
        return False

    trusted = [
        # big venues & organisers
        "excel.london",
        "olympia.london",
        "thenec.co.uk",
        "necgroup.co.uk",
        "eventbrite",
        "ticketmaster",
        "see.tickets",
        "kew.org",
        "goodwood.com",
        "bluewater.co.uk",
        "dreamland.co.uk",
        "alexandrapalace.com",
        "visitlondon.com",
        "hydeparkwinterwonderland.com",
        "winterlandbluewater.com",
        "lovefairs.com",
        "kenteventcentre.co.uk",
        "harrogateconventioncentre.co.uk",
        "brighton",
        "harrogate",
    ]
    u = url.lower()
    return any(t in u for t in trusted)


def filter_future_and_valid(events):
    """
    Keep only future-dated, plausible events.
    """
    today = datetime.date.today()
    out = []

    for ev in events or []:
        try:
            raw_date = ev.get("date", "") or ""
            first = raw_date.split(" ")[0]
            event_date = datetime.date.fromisoformat(first)

            # ignore past events
            if event_date < today:
                continue

            # very rough anti-hallucination guard:
            title = (ev.get("title") or "").lower()

            banned_titles = [
                "gaming expo",
                "tech & gaming",
                "winter tech expo",
                "london comics expo",
                "retro gaming expo",
            ]
            if any(b in title for b in banned_titles):
                continue

            url = ev.get("url", "") or ""

            # If URL looks bad and the title doesn't clearly mention
            # real-world style events, drop it.
            if url and not url_seems_real(url):
                recur = ["festival", "market", "fair", "christmas", "county show"]
                if not any(r in title for r in recur):
                    continue

            out.append(ev)
        except Exception:
            # any parsing failure → drop the event
            continue

    return out


# ------------------------------------------------------------------
# Main search entry point used by the FastAPI backend
# ------------------------------------------------------------------
def smart_event_search(region: str, keywords: str):
    """
    Core search function called by the FastAPI API.

    NEW BEHAVIOUR:
    - Treats rules.txt as a curated text catalogue of REAL events.
    - GPT is explicitly forbidden from inventing any events.
    - Only events that appear in rules.txt (Event Data section),
      pins.json or seed_events.json may be returned.
    """

    # Load remote configuration / data
    rules_text = load_remote(RULES_URL)   # contains SYSTEM RULES + EVENT DATA
    notes_text = load_remote(NOTES_URL)   # still available if you want to use it later
    pins_json  = load_json_remote(PINS_URL)
    seeds_json = load_json_remote(SEED_URL)

    today = datetime.date.today().isoformat()

    # ------------------------------------------------------------------
    # Prompt: tell GPT that rules.txt is the ground-truth event list
    # ------------------------------------------------------------------
    prompt = f"""
You are PopFinder, the UK's vendor opportunity engine.

You are given:
1) The full contents of a file called rules.txt, which contains:
   - A SYSTEM RULES section (instructions)
   - An EVENT DATA section, which is a manually curated list of REAL,
     UPCOMING events, generated by another GPT run.
2) A JSON array of pinned events (PINNED_EVENTS_JSON) which are also
   real and should be treated as trusted events.

IMPORTANT CONSTRAINTS (DO NOT BREAK):
- You MUST treat the EVENT DATA part of rules.txt as the
  **only authoritative catalogue of upcoming events**.
- You are STRICTLY FORBIDDEN from inventing or hallucinating events,
  titles, dates, venues, or URLs.
- If an event is NOT clearly present in EVENT DATA or in
  PINNED_EVENTS_JSON, you MUST NOT output it.
- Region and keyword filters MUST NEVER cause you to create new events.
  They can only be used to INCLUDE or EXCLUDE events from this catalogue.

USER QUERY:
- Region: {region}
- Keywords: {keywords}
- Today: {today}

rules.txt CONTENT (SYSTEM RULES + EVENT DATA):
<<<RULES_TXT_START
{rules_text}
RULES_TXT_END>>>

PINNED_EVENTS_JSON (trusted structured events):
{json.dumps(pins_json, ensure_ascii=False)}

TASK:
1. Read rules.txt carefully and identify ALL events explicitly described
   in the EVENT DATA section.
2. Combine those events with the events already present in
   PINNED_EVENTS_JSON.
3. For this user query, keep ONLY events that:
   - Match the requested region:
       * If region is "UK" or "UK – National", keep UK-wide events.
       * If region is a city/area (e.g. "London", "Kent"), keep events
         clearly located there or strongly associated with that area.
   - AND (if the keywords string is non-empty) contain at least one
     keyword (case-insensitive) in the title, description or location.

4. For EACH kept event, output an object using EXACTLY this schema:
   {{
     "title": "",
     "date": "",
     "location": "",
     "description": "",
     "url": "",
     "category": "",
     "footfall_score": 0,
     "vendor_fit_score": 0
   }}

   Notes:
   - Do NOT guess unknown fields. If a field is missing in the source
     text, you may leave it as an empty string or 0.
   - You may normalise dates into ISO form
     ("YYYY-MM-DD" or "YYYY-MM-DD to YYYY-MM-DD") but must NOT change
     the actual meaning (no new dates, no new ranges).
   - URL must come exactly from the text or JSON, not invented.

5. Return a SINGLE JSON array of these event objects.
   - Do NOT wrap it in Markdown.
   - Do NOT add any explanation, commentary or prose.
   - The response must be valid JSON that json.loads() can parse.
"""

    # ------------------------------------------------------------------
    # Call OpenAI
    # ------------------------------------------------------------------
    response = client.responses.create(
        model="gpt-4.1-mini",
        input=prompt,
        max_output_tokens=3000,
    )

    # ------------------------------------------------------------------
    # Parse JSON; on failure, fall back to seeds only
    # ------------------------------------------------------------------
    try:
        llm_events = json.loads(response.output_text)
        if not isinstance(llm_events, list):
            raise ValueError("LLM output was not a JSON array")
    except Exception:
        # If GPT output isn't valid JSON, just fall back to curated seeds
        return filter_future_and_valid(seeds_json)

    # ------------------------------------------------------------------
    # Clean & merge with seeds (as extra, low-priority events)
    # ------------------------------------------------------------------
    cleaned_llm   = filter_future_and_valid(llm_events)
    future_seeds  = filter_future_and_valid(seeds_json)

    # Order of priority:
    #   1) Events parsed from rules.txt (cleaned_llm)
    #   2) Seed events (future_seeds) as backup / extras
    merged = cleaned_llm + future_seeds

    # De-duplicate by (title, date, location)
    unique = []
    seen = set()

    for ev in merged:
        title = (ev.get("title") or "").strip().lower()
        date  = (ev.get("date") or "").strip().lower()
        loc   = (ev.get("location") or "").strip().lower()
        key = (title, date, loc)

        if not title:
            # discard totally empty shells
            continue

        if key in seen:
            continue

        seen.add(key)
        unique.append(ev)

    return unique
